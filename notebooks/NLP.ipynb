{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notes for Chapter 10: NLP Deep Dive: RNNs\n",
    "=========================================\n",
    "\n",
    "Introduction & Background\n",
    "-------------------------\n",
    "\n",
    "A *language model* is a \"model that has been trained to guess the next\n",
    "word in a text\" after reading the preceding words. This is called\n",
    "\"self-supervised learning,\" we just need to give the model a lot of text\n",
    "to work with. Self-supervised learning is usually used to *pre-train* a\n",
    "model for later use in transfer learning. The general process we will be\n",
    "following is:\n",
    "\n",
    "-   Concatenate all of the documents in our dataset into one big long\n",
    "    string and split it into words (tokens)\n",
    "-   Independent variable is first word to second-to-last word. Dependent\n",
    "    variable is second word to last word.\n",
    "-   Vocab will comprise common words already in vocab of pretrained\n",
    "    model, and application-specific words. We will initialize new\n",
    "    embedding matrix rows for these new vocab words.\n",
    "\n",
    "Some relevant terminology:\n",
    "\n",
    "-   *Tokenization* is the process of converting text into a list of\n",
    "    words (or characters or substrings)\n",
    "-   *Numericalization* means listing all of the unique words that appear\n",
    "    in the vocab and converting them to a number by looking up the index\n",
    "    in the vocab\n",
    "-   *Data Loader Creation*: We use the `LMDataLoader` class to create\n",
    "    independent and dependent variables offset from each other by one\n",
    "    token.\n",
    "-   *Language Model Creation*: Recurrent neural network; details to\n",
    "    follow.\n",
    "\n",
    "We will go through each of these steps separately.\n",
    "\n",
    "### Tokenization\n",
    "\n",
    "There are many approaches to tokenization.\n",
    "\n",
    "-   Word-based tokenization splits by spaces and also applies\n",
    "    language-specific rules to separate e.g. don't into do n't to\n",
    "    capture meaning.\n",
    "-   Subword-based tokenization splits words into smaller parts based on\n",
    "    common substrings.\n",
    "-   Character-based tokenization splits a sentence into individual\n",
    "    characters.\n",
    "\n",
    "1.  Word Tokenization in Fastai\n",
    "\n",
    "    Fastai provides an interface to a range of different tokenizers in\n",
    "    external libraries (it does not provide its own tokenizers). We're\n",
    "    going to experiment with the IMDb dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(#7) [Path('/home/djliden91/.fastai/data/imdb/test'),Path('/home/djliden91/.fastai/data/imdb/imdb.vocab'),Path('/home/djliden91/.fastai/data/imdb/tmp_clas'),Path('/home/djliden91/.fastai/data/imdb/train'),Path('/home/djliden91/.fastai/data/imdb/unsup'),Path('/home/djliden91/.fastai/data/imdb/tmp_lm'),Path('/home/djliden91/.fastai/data/imdb/README')]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "    from fastai.text.all import *\n",
    "    path = untar_data(URLs.IMDB)\n",
    "    path.ls()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Now we need to get the text files themselves.\n",
    "\n",
    "    ``` jupyter-python\n",
    "    files = get_text_files(path, folders=['train','test','unsup'])\n",
    "    txt = files[0].open().read(); txt[:75]\n",
    "    ```\n",
    "\n",
    "    Now, we demonstrate the tokenization functions.\n",
    "\n",
    "    ``` jupyter-python\n",
    "    spacy = WordTokenizer()\n",
    "    toks = first(spacy([txt]))\n",
    "    print(coll_repr(toks,30))\n",
    "    ```\n",
    "\n",
    "    The default tokenizer used by `fastai` at the time of writing is\n",
    "    `spaCy`. It identifies and handles specific language cases pretty\n",
    "    well. For example:\n",
    "\n",
    "    ``` jupyter-python\n",
    "    first(spacy(['The U.S. dollar $1 is $1.00.']))\n",
    "    ```\n",
    "\n",
    "    We can apply some more specific options and tokenization formats\n",
    "    using the `fastai` tokenizer class.\n",
    "\n",
    "    ``` jupyter-python\n",
    "    tkn = Tokenizer(spacy)\n",
    "    print(coll_repr(tkn(txt), 31))\n",
    "    ```\n",
    "\n",
    "    In this case, tokens preceded by `xx` are special tokens. `xxbos`\n",
    "    indicates the start of a new text, for example. Other common ones:\n",
    "\n",
    "    -   `xxmaj` indicates the following word starts with a capital\n",
    "    -   `xxunk` represents that the next word is unknown\n",
    "\n",
    "    We can check on the default rules as follows\n",
    "\n",
    "    ``` jupyter-python\n",
    "    defaults.text_proc_rules\n",
    "    ```\n",
    "\n",
    "    1.  Subword Tokenization\n",
    "\n",
    "        Particularly with languages such as Japanese or Chinese, spaces\n",
    "        do not provide a good guide to divisions between words. Other\n",
    "        languages group many \"subwords\" together into a single word, but\n",
    "        the subwords themselves can contain various separate meanings.\n",
    "        Subword tokenization can deal with these sorts of situations.\n",
    "        This is a two-step process:\n",
    "\n",
    "        1.  Analyze a corpus of documents. Find the most commonly\n",
    "            occurring groups of letters. These are the vocab.\n",
    "        2.  Tokenize using the subword vocab from (1)\n",
    "\n",
    "        Let's try it out.\n",
    "\n",
    "        ``` jupyter-python\n",
    "        txts = L(o.open().read() for o in files[:2000])\n",
    "\n",
    "        def subword(sz):\n",
    "            sp = SubwordTokenizer(vocab_sz=sz)\n",
    "            sp.setup(txts)\n",
    "            return ' '.join(first(sp([txt]))[:40])\n",
    "\n",
    "        subword(1000)\n",
    "        ```\n",
    "\n",
    "        ``` example\n",
    "        ▁what ▁can ▁i ▁say , ▁this ▁film ▁is ▁amazing . ▁it ▁has ▁its ▁fla w s ▁like ▁every ▁film ▁does ▁for ▁example ▁w o b b ly ▁he ad st one s ▁in ▁a ▁gr a ve y ard ,\n",
    "        ```\n",
    "\n",
    "        The special character `_` represents a space in the original\n",
    "        text.\n",
    "\n",
    "        The number we passed to `subword` represents the size of the\n",
    "        vocab. We can use a smaller vocab:\n",
    "\n",
    "        ``` jupyter-python\n",
    "        subword(200)\n",
    "        ```\n",
    "\n",
    "        In this case, each token represents fewer characters, so it\n",
    "        takes more tokens to represent the same sentence. We can also\n",
    "        see what happens when we use a *larger* vocab.\n",
    "\n",
    "        ``` jupyter-python\n",
    "        subword(10000)\n",
    "        ```\n",
    "\n",
    "        Here we see that our vocab is coming closer to capturing full\n",
    "        words. What considerations guide the choice of vocab size?\n",
    "        Smaller vocab means smaller embedding matrix and requires less\n",
    "        data to learn. A larger vocab will require a larger embedding\n",
    "        matrix and thus more data, but it means fewer token per\n",
    "        sentence, which translates to faster training, less memory, and\n",
    "        fewer states for the model to remember.\n",
    "\n",
    "    2.  Numericalization with fastai\n",
    "\n",
    "        The next step is to map tokens to integers. We can do this as\n",
    "        follows:\n",
    "\n",
    "        ``` jupyter-python\n",
    "        # Revisiting our Tokenizer from before\n",
    "        toks = tkn(txt)\n",
    "        print(coll_repr(tkn(txt),31))\n",
    "        ```\n",
    "\n",
    "        ``` jupyter-python\n",
    "        # Prepare a subset for numericalization\n",
    "        toks200 = txts[:200].map(tkn)\n",
    "        toks200[0]\n",
    "        ```\n",
    "\n",
    "        We apply the numericalization with the `Numericalize` class.\n",
    "\n",
    "        ``` jupyter-python\n",
    "        # Numericalize\n",
    "        # Lists words -- first special tokens, then in frequency order\n",
    "        num = Numericalize()\n",
    "        num.setup(toks200)\n",
    "        coll_repr(num.vocab, 20)\n",
    "        ```\n",
    "\n",
    "        ``` jupyter-python\n",
    "        # print some text in numericalized form\n",
    "        nums = num(toks)[:20]; nums\n",
    "        ```\n",
    "\n",
    "        ``` jupyter-python\n",
    "        # Map back to original text\n",
    "        ' '.join(num.vocab[o] for o in nums)\n",
    "        ```\n",
    "\n",
    "        ``` example\n",
    "        xxbos what can i say , this film is amazing . it has its flaws like every film does for\n",
    "        ```\n",
    "\n",
    "    3.  Putting Texts into Batches for a Language Model\n",
    "\n",
    "        We can't just resize text to the desired dimensions as we could\n",
    "        with images. We want our batches to run in order, each picking\n",
    "        up where the last left off. Another challenge is that language\n",
    "        models typically include a large number of tokens – likely more\n",
    "        than can fit in GPU memory. At each epoch, we (1) shuffle our\n",
    "        collection of documents; (2) concatenate them into a stream of\n",
    "        tokens; (3) cut that stream into a batch of fixed-size\n",
    "        mini-streams in order.\n",
    "\n",
    "        Let's make our dataloader and take a look at one batch:\n",
    "\n",
    "        ``` jupyter-python\n",
    "        # numericalize\n",
    "        nums200 = toks200.map(num)\n",
    "\n",
    "        # pass to LMDataloader\n",
    "        dl = LMDataLoader(nums200)\n",
    "\n",
    "        # check results by looking at first batch\n",
    "        x,y = first(dl)\n",
    "        x.shape, y.shape\n",
    "\n",
    "        # Look at first row of independent variable\n",
    "        ' '.join(num.vocab[o] for o in x[0][:20])\n",
    "        ```\n",
    "\n",
    "        Now we check out the dependent variable. Note that it is offset\n",
    "        from the independent variable by one position.\n",
    "\n",
    "        ``` jupyter-python\n",
    "        # Look at dependent variable\n",
    "        # same as independent but offset by one\n",
    "\n",
    "        ' '.join(num.vocab[o] for o in y[0][:20])\n",
    "        ```\n",
    "\n",
    "    4.  The easier way of preprocessing: DataBlock\n",
    "\n",
    "        We can, of course, use the high-level DataBlock API to prepare\n",
    "        our data for the model. Specifically, we use a `TextBlock`.\n",
    "\n",
    "        ``` jupyter-python\n",
    "        # get items function\n",
    "        get_imdb = partial(get_text_files, folders=['train','test','unsup'])\n",
    "\n",
    "        # datablock\n",
    "        dls_lm = DataBlock(\n",
    "            blocks=TextBlock.from_folder(path, is_lm=True),\n",
    "            get_items=get_imdb, splitter=RandomSplitter(0.1)\n",
    "            ).dataloaders(path, path=path, bs=128, seq_len=80)\n",
    "\n",
    "        dls_lm.show_batch(max_n=2)\n",
    "        ```\n",
    "\n",
    "        `TextBlock` implements a few efficiency optimizations:\n",
    "\n",
    "        -   saves the tokenized documents in a temp folder so it doesn't\n",
    "            need to do it more than once\n",
    "        -   runs processes in parallel to take advantage of multiple\n",
    "            CPUs.\n",
    "\n",
    "2.  Fine-Tune the Language Model\n",
    "\n",
    "    We will use a recurrent neural network (RNN) with an architecture\n",
    "    called \"AWD-LSTM\". In this architecture, embeddings in the\n",
    "    pretrained model are merged with random embeddings added for words\n",
    "    *not* in the original vocab. The learner handles this automatically.\n",
    "\n",
    "    ``` jupyter-python\n",
    "    learn = language_model_learner(\n",
    "        dls_lm, AWD_LSTM, drop_mult=0.3,\n",
    "        metrics=[accuracy, Perplexity()]) #.to_fp16() requires GPU\n",
    "    ```\n",
    "\n",
    "    -   cross-entropy loss is used. We basically have a classification\n",
    "        problem; the different categories are the words in our vocab.\n",
    "    -   The perplexity metric is the exponential of the loss.\n",
    "\n",
    "    It takes a long time to train each epoch, so we go one at a time and\n",
    "    save the in-between results.\n",
    "\n",
    "    ``` jupyter-python\n",
    "    learn.fit_one_cycle(1, 2e-2)\n",
    "    learn.reco\n",
    "    ```\n",
    "\n",
    "    :end:\n",
    "\n",
    "    And that killed the kernel, so we'll work on this later.\n",
    "\n",
    "    ``` jupyter-python\n",
    "    ```\n",
    "\n",
    "Testing Jupyter Mode Source Blocks\n",
    "==================================\n",
    "\n",
    "``` jupyter-python\n",
    "x = 'foo'\n",
    "y = 'bar'\n",
    "x + ' ' + y\n",
    "```\n",
    "\n",
    "> What does `(shell-command-to-string \"jupyter kernelspec list\")`\n",
    "> return?\n",
    ">\n",
    "> If it doesn't fail and returns the kernelspecs, then it might just be\n",
    "> that by the time the ob-jupyter file is loaded (which is when we try\n",
    "> to get the kernelspecs) the paths used by Emacs to search for shell\n",
    "> programs aren't setup yet. If this is the case you should be able to\n",
    "> call `(org-babel-jupyter-aliases-from-kernelspecs)` to get everything\n",
    "> working again.\n",
    ">\n",
    "> The ob-jupyter file is loaded whenever that\n",
    "> org-babel-do-load-languages call is evaluated so you should check to\n",
    "> see that (executable-find \"jupyter\") returns a valid path right before\n",
    "> the call.\n",
    "\n",
    "OK! So that works, but it looks like there were some problems in the\n",
    "order in which I started my virtual environment compared to when I\n",
    "started emacs. Which makes sense given that I am using emacs-daemon, so\n",
    "emacs is initialized well before I start my virtual environment. Now,\n",
    "onto business!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
