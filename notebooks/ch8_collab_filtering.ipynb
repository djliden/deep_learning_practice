{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collaborative Filtering\n",
    "Problem setting: there are many products and many users. In collaborative filtering, we look at the products the current user has used or liked, find others who have liked similar products, and recommend products that those other users have used or liked.\n",
    "\n",
    "## Latent Factors\n",
    "The foundational idea is that of \"latent factors.\" We don't really need to know much of anything about the users or about the products except for which users have used/liked which products. But we assume there are some underlying unifying characteristics about these users and/or products. Netflix could recommend a bunch of '70s sci-fi without possessing any concept of '70s sci-fi. But that underlying concept is still what we are getting at.\n",
    "\n",
    "## The Data \n",
    "We will use a subset of the `MovieLens` dataset. The full dataset contains tens of millions of movie rankings (a rating, a movie ID, and a user ID). We will use a subset of 100,000 of these rankings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.collab import *\n",
    "from fastai.tabular.all import *\n",
    "path = untar_data(URLs.ML_100k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user</th>\n",
       "      <th>movie</th>\n",
       "      <th>rating</th>\n",
       "      <th>timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>196</td>\n",
       "      <td>242</td>\n",
       "      <td>3</td>\n",
       "      <td>881250949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>186</td>\n",
       "      <td>302</td>\n",
       "      <td>3</td>\n",
       "      <td>891717742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>22</td>\n",
       "      <td>377</td>\n",
       "      <td>1</td>\n",
       "      <td>878887116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>244</td>\n",
       "      <td>51</td>\n",
       "      <td>2</td>\n",
       "      <td>880606923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>166</td>\n",
       "      <td>346</td>\n",
       "      <td>1</td>\n",
       "      <td>886397596</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user  movie  rating  timestamp\n",
       "0   196    242       3  881250949\n",
       "1   186    302       3  891717742\n",
       "2    22    377       1  878887116\n",
       "3   244     51       2  880606923\n",
       "4   166    346       1  886397596"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ratings = pd.read_csv(path/'u.data', delimiter='\\t', header=None,\n",
    "                     names=['user', 'movie', 'rating', 'timestamp'])\n",
    "ratings.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compared to the full dataset, this subset contains some of the most popular (most reviewed) movies, and some of the most prolific reviewers of movies. What we are ultimately interested in is being able to assign predicted ratings to user-movie combinations that do not have ratings.\n",
    "\n",
    "## Learning Latent Factors\n",
    "We can imagine a circumstance where each movie is rated according to a handful of factors, and each user has a certain proclivity for those factors. Maybe we have `['recency', 'high action', 'long movie']`. A movie with values `[-0.9, 0, 0.7]` would be an old, medium-action, long movie. A user's preferences can be expressed in the same way. A user who likes new, high-action, short movies could be represented with the list `[0.8, 0.9, -0.7]`. We could determine how likely the user is to like our old movie by taking the dot product. We end up with -1.21 -- a poor match (quality of match here could range from -3 to 3).\n",
    "\n",
    "In the model of interest in this chapter, though, we want the *model* to learn the latent factors so we don't have to. That involves the following steps:\n",
    "\n",
    "1. Randomly initialize some parameters. These parameters are a set of latent factors for each movie and user. We need to choose how many to use, but we do *not* choose what they mean.\n",
    "2. Calculate predictions. We can take the dot products of the parameters for each movie and user to assign a predicted score to each movie/user combination.\n",
    "3. Calculate the loss. We can use any loss function; for now we'll use MSE for simplicity. With these steps in place, we can optimize our parameters with SGD. \n",
    "\n",
    "My question at this phase is: how do we choose to optimize viewer vs. movie parameters? Is there any meaningful difference?\n",
    "\n",
    "## Preparing the DataLoaders\n",
    "We start by getting movie titles and corresponding IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>movie</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Toy Story (1995)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>GoldenEye (1995)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Four Rooms (1995)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Get Shorty (1995)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Copycat (1995)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   movie              title\n",
       "0      1   Toy Story (1995)\n",
       "1      2   GoldenEye (1995)\n",
       "2      3  Four Rooms (1995)\n",
       "3      4  Get Shorty (1995)\n",
       "4      5     Copycat (1995)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movies = pd.read_csv(path/'u.item', delimiter='|', encoding='latin-1',\n",
    "                    usecols=(0,1), names=('movie', 'title'), header=None)\n",
    "movies.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user</th>\n",
       "      <th>movie</th>\n",
       "      <th>rating</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>196</td>\n",
       "      <td>242</td>\n",
       "      <td>3</td>\n",
       "      <td>881250949</td>\n",
       "      <td>Kolya (1996)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>63</td>\n",
       "      <td>242</td>\n",
       "      <td>3</td>\n",
       "      <td>875747190</td>\n",
       "      <td>Kolya (1996)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>226</td>\n",
       "      <td>242</td>\n",
       "      <td>5</td>\n",
       "      <td>883888671</td>\n",
       "      <td>Kolya (1996)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>154</td>\n",
       "      <td>242</td>\n",
       "      <td>3</td>\n",
       "      <td>879138235</td>\n",
       "      <td>Kolya (1996)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>306</td>\n",
       "      <td>242</td>\n",
       "      <td>5</td>\n",
       "      <td>876503793</td>\n",
       "      <td>Kolya (1996)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user  movie  rating  timestamp         title\n",
       "0   196    242       3  881250949  Kolya (1996)\n",
       "1    63    242       3  875747190  Kolya (1996)\n",
       "2   226    242       5  883888671  Kolya (1996)\n",
       "3   154    242       3  879138235  Kolya (1996)\n",
       "4   306    242       5  876503793  Kolya (1996)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Merge with ratings\n",
    "ratings = ratings.merge(movies, on = \"movie\")\n",
    "ratings.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user</th>\n",
       "      <th>title</th>\n",
       "      <th>rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>650</td>\n",
       "      <td>Bridges of Madison County, The (1995)</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>616</td>\n",
       "      <td>Mad City (1997)</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>167</td>\n",
       "      <td>Mr. Smith Goes to Washington (1939)</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>224</td>\n",
       "      <td>Phenomenon (1996)</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>327</td>\n",
       "      <td>Scream (1996)</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>10</td>\n",
       "      <td>Sabrina (1954)</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>56</td>\n",
       "      <td>Multiplicity (1996)</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>311</td>\n",
       "      <td>Fargo (1996)</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>721</td>\n",
       "      <td>Volcano (1997)</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>479</td>\n",
       "      <td>Princess Bride, The (1987)</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Build a DataLoaders\n",
    "dls = CollabDataLoaders.from_df(ratings, item_name='title', bs=64)\n",
    "dls.show_batch()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Doing this in PyTorch\n",
    "We can't use the pandas crosstab presentation directly. We need tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_users = len(dls.classes['user'])\n",
    "n_movies = len(dls.classes['title'])\n",
    "n_factors = 5 # Can adjust this however we want\n",
    "\n",
    "user_factors = torch.randn(n_users, n_factors)\n",
    "movie_factors = torch.randn(n_movies, n_factors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One challenge is how to look up our user-movie correspondences (i.e. to find the indices). To do this, we replace our indices with one-hot coded vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 0., 1., 0., 0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_hot_3 = one_hot(3, n_users).float()\n",
    "one_hot_3[1:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.9431, -1.8060, -1.6114, -0.8795, -0.2871])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_factors.t() @ one_hot_3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the same as if we used `user_factors[3]`. This vector-based approach resembles what is occurring behind the scenes. Behind the scences there is an \"embedding matrix\" that is multiplied to find indices.\n",
    "\n",
    "As discussed above, in this model, we're not specifying what the factors of interest are. We're letting the model figure it out for itself by working through the connections between users and movies. We'll be able to see what sorts of movies are \"grouped\" at the end and to isolate genres, blockbusters, etc.\n",
    "\n",
    "## Collaborative Filtering Model from Scratch\n",
    "(begins with a little review of classes). We will use the PyTorch `module` class to define our own dot product class. I updated the original version from p261 to include the addition of `y_range` from 262, ensuring our predictions fall between 0 and 5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DotProduct(Module):\n",
    "    def __init__(self, n_users, n_movies, n_factors, y_range = (0,5.5)):\n",
    "        self.user_factors = Embedding(n_users, n_factors)\n",
    "        self.movie_factors = Embedding(n_movies, n_factors)\n",
    "        self.y_range=y_range\n",
    "        \n",
    "    def forward(self, x):\n",
    "        users = self.user_factors(x[:,0])\n",
    "        movies = self.movie_factors(x[:,1])\n",
    "        # We defined y_range as a tuple so *self.y_range unpacks it.\n",
    "        return sigmoid_range((users * movies).sum(dim=1), *self.y_range)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.990264</td>\n",
       "      <td>0.984855</td>\n",
       "      <td>00:10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.841716</td>\n",
       "      <td>0.898693</td>\n",
       "      <td>00:09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.660927</td>\n",
       "      <td>0.865518</td>\n",
       "      <td>00:08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.462861</td>\n",
       "      <td>0.872845</td>\n",
       "      <td>00:08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.351368</td>\n",
       "      <td>0.878040</td>\n",
       "      <td>00:08</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = DotProduct(n_users, n_movies, 50)\n",
    "learn = Learner(dls, model, loss_func=MSELossFlat())\n",
    "learn.fit_one_cycle(5,5e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we now have a working model. What are the next steps? Some missing pieces:\n",
    "- Some viewers are just more positive/negative and some movies are just good/bad. How do we account for this?\n",
    "\n",
    "Answer: we currently just have weights. We also need a bias term. One bias term for each viewer and each movie will allow us to adjust for these considerations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1.009261</td>\n",
       "      <td>1.000437</td>\n",
       "      <td>00:11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.900416</td>\n",
       "      <td>0.898409</td>\n",
       "      <td>00:09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.699724</td>\n",
       "      <td>0.859263</td>\n",
       "      <td>00:08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.487397</td>\n",
       "      <td>0.864799</td>\n",
       "      <td>00:10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.358212</td>\n",
       "      <td>0.869992</td>\n",
       "      <td>00:12</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "class DotProductBias(Module):\n",
    "    def __init__(self, n_users, n_movies, n_factors, y_range=(0,5.5)):\n",
    "        self.user_factors = Embedding(n_users, n_factors)\n",
    "        self.user_bias = Embedding(n_users,1)\n",
    "        self.movie_factors = Embedding(n_movies, n_factors)\n",
    "        self.movie_bias = Embedding(n_movies,1)\n",
    "        self.y_range=y_range\n",
    "        \n",
    "    def forward(self, x):\n",
    "        users = self.user_factors(x[:,0])\n",
    "        movies = self.movie_factors(x[:,1])\n",
    "        res = (users*movies).sum(dim=1, keepdim=True)\n",
    "        res += self.user_bias(x[:,0]) + self.movie_bias(x[:,1])\n",
    "        return sigmoid_range(res, *self.y_range)\n",
    "    \n",
    "model = DotProduct(n_users, n_movies, 50)\n",
    "learn = Learner(dls, model, loss_func=MSELossFlat())\n",
    "learn.fit_one_cycle(5,5e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weight Decay\n",
    "Our model actually got worse by the end. Oops. Next time, we'll get into *weight decay* as a method of solving this. Weight decay is L2 regularization. In L2 regularization (ridge regression?) we add the sum of squared weights to the loss function. This discourages overfitting by encouraging lower weights in general. This can discourage overfitting by preventing the model from assigning very high weights associated with certain training situations (?).\n",
    "\n",
    "We can pass `wd` as an argument to `learn.fit_one_cycle`. Different values can have different results (we can vary the degree of regularization)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.957417</td>\n",
       "      <td>0.943936</td>\n",
       "      <td>00:10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.857454</td>\n",
       "      <td>0.866373</td>\n",
       "      <td>00:10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.708513</td>\n",
       "      <td>0.822341</td>\n",
       "      <td>00:10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.603905</td>\n",
       "      <td>0.810551</td>\n",
       "      <td>00:13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.478483</td>\n",
       "      <td>0.811706</td>\n",
       "      <td>00:09</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = DotProductBias(n_users, n_movies, 50)\n",
    "learn = Learner(dls, model, loss_func=MSELossFlat())\n",
    "learn.fit_one_cycle(5,5e-3,wd=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A definite improvement."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating our own embedding module\n",
    "We haven't thought much about what `embedding` actually is/does, so we're going to recreate it without using the class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(#0) []"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class T(Module):\n",
    "    def __init__(self): self.a = torch.ones(3)\n",
    "\n",
    "L(T().parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What does the above illustrate? Optimizers need to be able to get parameters from a module's `parameters` method, but this is not automatic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(#1) [Parameter containing:\n",
       "tensor([1., 1., 1.], requires_grad=True)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class T(Module):\n",
    "    def __init__(self): self.a = nn.Parameter(torch.ones(3))\n",
    "\n",
    "L(T().parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All `PyTorch` modules use `nn.Parameter` for any/all trainable parameters. For this reason, we have not needed to explicitly call `nn.Parameter`. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((#1) [Parameter containing:\n",
       " tensor([[-0.7359],\n",
       "         [-0.2971],\n",
       "         [ 0.4927]], requires_grad=True)],\n",
       " torch.nn.parameter.Parameter)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class T(Module):\n",
    "    def __init__(self): self.a = nn.Linear(1,3,bias=False)\n",
    "t = T()\n",
    "L(t.parameters()), type(t.a.weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a tensor as a parameter, with random initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_params(size):\n",
    "    return nn.Parameter(torch.zeros(*size).normal_(0,0.1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[ 0.1754, -0.1162, -0.0152, -0.0579],\n",
       "        [ 0.0509, -0.0623, -0.2051, -0.1134],\n",
       "        [-0.1045, -0.1003, -0.1089, -0.0824]], requires_grad=True)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "create_params([3,4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DotProductBias(Module):\n",
    "    def __init__(self, n_users, n_movies, n_factors, y_range=(0,5.5)):\n",
    "        self.user_factors = create_params([n_users, n_factors])\n",
    "        self.user_bias = create_params([n_users])\n",
    "        self.movie_factors = create_params([n_movies, n_factors])\n",
    "        self.movie_bias = create_params([n_movies])\n",
    "        self.y_range=y_range\n",
    "        \n",
    "    def forward(self, x):\n",
    "        users = self.user_factors[x[:,0]]\n",
    "        movies = self.movie_factors[x[:,1]]\n",
    "        res = (users*movies).sum(dim=1)\n",
    "        res += self.user_bias[x[:,0]] + self.movie_bias[x[:,1]]\n",
    "        return sigmoid_range(res, *self.y_range)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK, a quick reminder here. in `forward`, x refers to our input data. What do our input data look like? A two-column table of user IDs and movie IDs. Let's explore this step by step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "x,y = dls.one_batch()\n",
    "user_factors = create_params([n_users, n_factors])\n",
    "movie_factors = create_params([n_movies, n_factors])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([542, 551, 726, 833, 854, 868, 870, 533, 346, 541, 712, 385, 262, 916,\n",
       "        833, 658, 721, 823, 606, 468, 682, 489,  94, 308, 643,  63, 551, 593,\n",
       "        149, 248, 453, 399, 567,  81, 381, 382, 897, 151, 391, 296, 354, 763,\n",
       "        663,  16, 846, 801, 650, 268, 222, 142, 860, 405, 620, 347, 456, 176,\n",
       "        132, 148, 110, 130, 406, 757, 710,  13])"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xuser = x[:,0]\n",
    "xuser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([944, 5])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_factors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([64, 5]), torch.Size([64]))"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(user_factors[xuser]).shape, xuser.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0781,  0.0762,  0.1623,  0.1272, -0.1008],\n",
       "        [ 0.0773, -0.0719, -0.1299,  0.2710, -0.0511],\n",
       "        [-0.1808, -0.1901, -0.0663,  0.0985, -0.0308]],\n",
       "       grad_fn=<IndexBackward>)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_factors[[1,2,4]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So it looks like we're treating the user inputs as indices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(916)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xuser.max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK, so how does that work if we have 1649 as our max user but only 944 user_factors? Is x rescaled in some way? NO. We don't have 1649 users. `x.max` included the movie indices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1619)"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xmovie = x[:,1]\n",
    "xmovie.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1665, 5])"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movie_factors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([840, 339, 328, 198, 215, 816, 463, 719, 210, 912, 230, 454, 450,  95,\n",
       "        496, 899, 699, 405, 326, 671, 907, 271, 472,   5, 715, 389, 919, 406,\n",
       "        593, 374, 503,  69, 385,  24, 749, 497, 894,  18, 727, 506, 493, 207,\n",
       "        688, 181, 898, 798, 843,  59, 128,  59, 104, 795, 690, 437, 897, 908,\n",
       "        747, 440, 233, 658, 509, 846, 887,  56])"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xuser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.999838</td>\n",
       "      <td>0.984600</td>\n",
       "      <td>00:10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.804278</td>\n",
       "      <td>0.851267</td>\n",
       "      <td>00:09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.649053</td>\n",
       "      <td>0.841685</td>\n",
       "      <td>00:09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.493710</td>\n",
       "      <td>0.838438</td>\n",
       "      <td>00:09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.384005</td>\n",
       "      <td>0.840203</td>\n",
       "      <td>00:09</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = DotProductBias(n_users, n_movies, 50)\n",
    "learn = Learner(dls, model, loss_func = MSELossFlat())\n",
    "learn.fit_one_cycle(5, 5e-3, wd=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So this is in line with our earlier model. So what, then, is an embedding? What does this have to do with indexing? WELL. We have our used factors indexed to a user. We have our movie factors indexed to a movie. We have a batch with *the same number of users and movies* because it's the ratings assigned by each user to the corresponding movie. Let's go through the steps manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([944, 5]), torch.Size([1665, 5]))"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x,y = dls.one_batch()\n",
    "user_factors = create_params([n_users, n_factors])\n",
    "movie_factors = create_params([n_movies, n_factors])\n",
    "user_factors.shape, movie_factors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([64, 5]), torch.Size([64, 5]))"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "users = user_factors[x[:,0]]\n",
    "movies = movie_factors[x[:,1]]\n",
    "users.shape,movies.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So now we have objects of the same size. Excellent. We started out with x as a two-column table of users and correpsonding movies, and now we have `users` and `movies`, each of which contain the 1x5 parameter tensors. Multiplying them and taking the sum gives us a set of ratings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-8.1124e-04,  1.0243e-04, -4.5069e-03, -1.3149e-04, -1.4269e-03],\n",
       "        [-2.4590e-03, -2.9902e-04,  1.2950e-02,  5.6369e-03,  1.1046e-03],\n",
       "        [ 8.2924e-03, -2.2118e-03,  1.9761e-02,  2.7960e-02, -1.2623e-02],\n",
       "        [ 2.1267e-03, -1.2918e-02, -9.1278e-04,  6.3873e-04, -1.8735e-03],\n",
       "        [-7.5046e-05, -1.7126e-03, -3.6889e-03, -5.0859e-03, -2.7960e-04],\n",
       "        [ 4.4283e-03,  4.2261e-03,  2.9040e-03,  8.0094e-03, -1.1118e-03],\n",
       "        [ 7.3242e-03, -6.2349e-04,  9.1704e-03, -4.9636e-03,  1.1910e-03],\n",
       "        [ 7.0370e-03, -5.5093e-03,  2.3006e-02, -1.8674e-03, -3.8760e-03],\n",
       "        [ 4.1600e-03, -2.1118e-04, -3.4236e-02,  2.4941e-03,  5.3758e-03],\n",
       "        [-4.5163e-06,  1.3069e-03,  5.5442e-04, -2.9947e-02,  4.9258e-03],\n",
       "        [-2.0122e-03,  7.4862e-04, -1.5255e-03, -9.7495e-03, -2.2546e-04],\n",
       "        [-6.8505e-03,  8.5523e-03,  2.4345e-03,  7.7216e-03, -3.1324e-03],\n",
       "        [ 3.2652e-03,  2.7617e-02,  2.9281e-03,  9.2022e-03,  6.7675e-04],\n",
       "        [-4.4453e-03, -1.8856e-05, -5.3588e-04,  1.5458e-02, -7.8219e-04],\n",
       "        [-3.9473e-03, -9.2272e-03, -1.0112e-02,  1.9711e-02,  4.2942e-06],\n",
       "        [-1.1137e-03,  2.1053e-02,  2.1078e-02, -9.4737e-03, -8.7563e-03],\n",
       "        [ 1.5556e-02,  8.3095e-04, -1.3527e-02, -4.3297e-05,  1.5960e-03],\n",
       "        [ 2.4891e-03,  6.7817e-03,  1.1657e-02,  5.1797e-03,  3.5267e-03],\n",
       "        [-2.9905e-03,  6.7442e-03,  1.7638e-02, -1.3503e-02, -1.0954e-02],\n",
       "        [-1.3571e-03,  3.5330e-06,  7.8192e-03,  3.5863e-03,  2.3308e-03],\n",
       "        [-6.5423e-03, -2.1930e-02,  5.5204e-03, -5.2370e-03, -5.8399e-02],\n",
       "        [ 2.6548e-04, -5.8512e-04,  1.1243e-02,  1.1819e-03, -3.9159e-03],\n",
       "        [ 4.7018e-03,  6.3025e-03, -4.7457e-03,  6.3216e-03,  3.6589e-03],\n",
       "        [-3.5144e-03, -8.3699e-03, -8.6428e-04, -1.6820e-03, -9.0992e-04],\n",
       "        [ 1.1395e-03,  7.1493e-05, -1.6343e-03,  2.9714e-03, -2.6486e-05],\n",
       "        [ 2.5851e-03,  2.1731e-03, -3.8224e-03, -2.2624e-03, -1.6728e-02],\n",
       "        [-5.5127e-03,  5.6961e-04, -1.1603e-02, -1.4400e-04,  4.1035e-03],\n",
       "        [ 1.7886e-02, -1.6722e-02, -1.5807e-02, -6.8085e-03,  1.5878e-02],\n",
       "        [-1.0193e-02, -9.1684e-03,  1.5025e-03,  5.9671e-04,  3.1121e-04],\n",
       "        [-2.3625e-03, -2.2837e-03,  1.0257e-03, -5.1684e-03,  9.0591e-03],\n",
       "        [-1.3956e-03,  6.4663e-03,  9.9368e-03,  6.0099e-04,  4.7176e-03],\n",
       "        [-1.0116e-02,  1.2733e-02,  1.3748e-02, -2.0211e-03,  6.6199e-03],\n",
       "        [ 3.1855e-03, -1.0772e-03,  1.5205e-03, -6.4671e-03,  2.5369e-05],\n",
       "        [ 2.9627e-03, -7.1819e-03, -5.6155e-03,  8.0085e-04,  8.8890e-03],\n",
       "        [ 1.0899e-02, -1.0276e-03,  1.8628e-03, -3.4695e-03,  1.5403e-02],\n",
       "        [ 8.4998e-04, -6.7704e-04,  2.4443e-03,  4.4573e-04,  2.2109e-03],\n",
       "        [-2.1279e-04,  2.6829e-03, -2.9727e-02,  2.3242e-02,  1.7032e-04],\n",
       "        [ 9.3072e-04,  1.5622e-05,  2.2047e-03,  1.9264e-02,  4.6761e-03],\n",
       "        [ 3.4901e-03,  1.7527e-08, -5.9639e-04,  1.4638e-02, -5.6309e-03],\n",
       "        [-5.8723e-03, -5.2277e-04,  5.1162e-04,  1.6545e-04,  3.4084e-02],\n",
       "        [-3.5114e-03,  2.9410e-03,  5.4850e-03,  1.0338e-03, -2.6653e-03],\n",
       "        [ 1.2413e-03, -1.4119e-03,  1.5393e-02, -1.9929e-03,  8.0069e-03],\n",
       "        [ 9.3415e-04, -7.6631e-03,  1.1833e-03, -4.2938e-03, -1.6347e-02],\n",
       "        [-1.1787e-02, -1.0120e-04, -1.1983e-03,  5.9409e-04,  8.8234e-04],\n",
       "        [-1.6330e-02,  7.3643e-03,  6.5808e-03, -1.7064e-03,  4.0245e-03],\n",
       "        [ 9.0354e-03,  1.1457e-03,  1.1132e-03, -3.3260e-02, -1.1026e-02],\n",
       "        [ 1.8786e-03, -8.3717e-04, -2.5067e-05, -2.8746e-04,  8.4039e-03],\n",
       "        [-1.0609e-02, -4.6700e-02,  1.1952e-04,  3.6458e-03,  1.2598e-03],\n",
       "        [ 1.5080e-02,  7.3361e-03, -1.7072e-02,  4.2371e-03,  3.2651e-03],\n",
       "        [ 4.2811e-03, -1.0766e-02,  8.6697e-03, -1.1145e-04, -5.0219e-04],\n",
       "        [ 3.8014e-05,  3.0848e-02, -9.1222e-03, -6.3266e-04,  8.6742e-03],\n",
       "        [ 6.8613e-04,  9.2252e-03,  1.1574e-03, -3.6113e-03,  1.6183e-04],\n",
       "        [ 4.5387e-03, -1.3468e-02, -3.6136e-03,  2.6640e-03, -9.5963e-04],\n",
       "        [-1.2195e-03, -1.1402e-03, -1.0115e-02, -1.5985e-02, -1.6868e-03],\n",
       "        [-1.1204e-03,  7.8859e-03,  1.7881e-03, -1.8568e-03, -3.6754e-03],\n",
       "        [-2.4712e-02,  1.5125e-03, -7.0281e-04, -9.0547e-04, -2.1538e-03],\n",
       "        [-1.1714e-02,  1.0981e-04, -4.8785e-04, -2.6925e-03, -1.1620e-03],\n",
       "        [-2.6220e-03, -5.7871e-03, -2.6099e-03,  1.5036e-02,  9.1661e-03],\n",
       "        [ 2.1602e-02, -2.8957e-03,  1.9065e-03, -1.8632e-03, -6.6092e-03],\n",
       "        [ 1.8275e-04, -4.3951e-03,  2.9636e-03, -2.3026e-04,  1.3866e-03],\n",
       "        [ 2.5656e-03, -1.4215e-03, -7.5319e-03,  9.3948e-03, -1.5059e-03],\n",
       "        [-5.1847e-04,  1.4928e-03, -8.6482e-03,  1.4074e-02, -9.3664e-04],\n",
       "        [-1.5288e-02,  4.5951e-04, -4.2060e-02, -5.0278e-03,  1.8456e-02],\n",
       "        [ 6.3497e-03,  1.2981e-03,  5.2563e-04, -4.9923e-03,  1.2665e-03]],\n",
       "       grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res = (users*movies)\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-6.7741e-03,  1.6934e-02,  4.1178e-02, -1.2939e-02, -1.0842e-02,\n",
       "         1.8456e-02,  1.2099e-02,  1.8791e-02, -2.2417e-02, -2.3164e-02,\n",
       "        -1.2764e-02,  8.7255e-03,  4.3690e-02,  9.6755e-03, -3.5715e-03,\n",
       "         2.2787e-02,  4.4123e-03,  2.9634e-02, -3.0653e-03,  1.2383e-02,\n",
       "        -8.6587e-02,  8.1895e-03,  1.6239e-02, -1.5341e-02,  2.5216e-03,\n",
       "        -1.8055e-02, -1.2586e-02, -5.5735e-03, -1.6950e-02,  2.7017e-04,\n",
       "         2.0326e-02,  2.0964e-02, -2.8130e-03, -1.4487e-04,  2.3668e-02,\n",
       "         5.2738e-03, -3.8443e-03,  2.7091e-02,  1.1901e-02,  2.8366e-02,\n",
       "         3.2831e-03,  2.1236e-02, -2.6186e-02, -1.1610e-02, -6.7135e-05,\n",
       "        -3.2992e-02,  9.1328e-03, -5.2284e-02,  1.2847e-02,  1.5716e-03,\n",
       "         2.9806e-02,  7.6193e-03, -1.0838e-02, -3.0146e-02,  3.0214e-03,\n",
       "        -2.6961e-02, -1.5946e-02,  1.3183e-02,  1.2140e-02, -9.2357e-05,\n",
       "         1.5011e-03,  5.4635e-03, -4.3460e-02,  4.4477e-03],\n",
       "       grad_fn=<SumBackward1>)"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res.sum(dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that `dim` specifies the dimension to *reduce*. So here we're reducing the columns and keeping one term per row. And now we have one rating for each row -- that is, for each movie/user combination in the batch.\n",
    "\n",
    "Question -- are there not multiple reviews per user? How are user weights kept consistent? Answer -- users are treated as indices and the weight objects exist as totally separate objects."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interpreting Embeddings and Biases\n",
    "Let's explore what our model has actually told us. First, we'll look at the lowest biases. These represent cases where, *even when users are well-matched to the other latent factors*, they still tend to assign low ratings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Children of the Corn: The Gathering (1996)',\n",
       " 'Grease 2 (1982)',\n",
       " 'Mortal Kombat: Annihilation (1997)',\n",
       " 'Lawnmower Man 2: Beyond Cyberspace (1996)',\n",
       " 'Kansas City (1996)']"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movie_bias = learn.model.movie_bias.squeeze()\n",
    "idxs = movie_bias.argsort()[:5]\n",
    "[dls.classes['title'][i] for i in idxs]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the highest:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Titanic (1997)',\n",
       " 'Shawshank Redemption, The (1994)',\n",
       " 'Rear Window (1954)',\n",
       " 'Good Will Hunting (1997)',\n",
       " 'Silence of the Lambs, The (1991)']"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idxs = movie_bias.argsort()[-5:]\n",
    "[dls.classes['title'][i] for i in idxs.flip(0)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Easy Way\n",
    "As usual, there is a way to do all of this using the `fastai` API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.970786</td>\n",
       "      <td>0.944023</td>\n",
       "      <td>00:10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.851556</td>\n",
       "      <td>0.869246</td>\n",
       "      <td>00:10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.733228</td>\n",
       "      <td>0.829357</td>\n",
       "      <td>00:10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.602670</td>\n",
       "      <td>0.814296</td>\n",
       "      <td>00:10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.486138</td>\n",
       "      <td>0.815382</td>\n",
       "      <td>00:10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn = collab_learner(dls, n_factors=50, y_range=(0,5.5))\n",
    "learn.fit_one_cycle(5,5e-3,wd=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EmbeddingDotBias(\n",
       "  (u_weight): Embedding(944, 50)\n",
       "  (i_weight): Embedding(1665, 50)\n",
       "  (u_bias): Embedding(944, 1)\n",
       "  (i_bias): Embedding(1665, 1)\n",
       ")"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learn.model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can replicate the \"most popular movie\" analysis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Titanic (1997)',\n",
       " 'Good Will Hunting (1997)',\n",
       " 'L.A. Confidential (1997)',\n",
       " 'Silence of the Lambs, The (1991)',\n",
       " 'Shawshank Redemption, The (1994)']"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movie_bias = learn.model.i_bias.weight.squeeze()\n",
    "idxs = movie_bias.argsort(descending=True)[:5]\n",
    "[dls.classes['title'][i] for i in idxs]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding Distance\n",
    "We can measure the \"distance\" between embedding vectors, basically with Euclidean distance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(#4) ['Man in the Iron Mask, The (1998)','8 Seconds (1994)','Crimson Tide (1995)','Boys of St. Vincent, The (1993)']"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movie_factors = learn.model.i_weight.weight\n",
    "idx = dls.classes['title'].o2i['Titanic (1997)']\n",
    "distances = nn.CosineSimilarity(dim=1)(movie_factors, movie_factors[idx][None])\n",
    "idx = distances.argsort(descending=True)[1:5]\n",
    "dls.classes['title'][idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bootstrapping the collab filtering model\n",
    "\n",
    "The *bootstrapping* problem occurs when you have no users & no data to learn from. Or, similarly, limited data. Or, perhaps more commonly, what about when you have a brand new user? Some solutions involve:\n",
    "- Use some \"average\" values\n",
    "- Try to obtain and use some Metadata.\n",
    "\n",
    "I am curious -- using metadata, given we have *latent* factors, how would we use the metadata to assign starting values?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning for Collaborative Filtering\n",
    "I guess the above wasn't really a DL model? Was not multi-layer. So we'd need to get the results of the embedding lookup and concatenate them. This returns something we can pass through the series of linear layers and nonlinearities in the \"usual way.\"\n",
    "\n",
    "We can use different numbers of embedding factors since we'll be concatenating the matrices, not taking the dot products. Fastai has a function recommending sizes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(944, 74), (1665, 102)]"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embs = get_emb_sz(dls)\n",
    "embs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CollabNN(Module):\n",
    "    def __init__(self, user_sz, item_sz, y_range=(0,5.5), n_act=100):\n",
    "        self.user_factors = Embedding(*user_sz)\n",
    "        self.item_factors = Embedding(*item_sz)\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(user_sz[1]+item_sz[1], n_act),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(n_act,1))\n",
    "        self.y_range=y_range\n",
    "        \n",
    "    def forward(self,x):\n",
    "        embs = self.user_factors(x[:,0]), self.item_factors(x[:,1])\n",
    "        x = self.layers(torch.cat(embs, dim=1))\n",
    "        return sigmoid_range(x, *self.y_range)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.956527</td>\n",
       "      <td>0.956170</td>\n",
       "      <td>00:12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.891190</td>\n",
       "      <td>0.902305</td>\n",
       "      <td>00:15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.841123</td>\n",
       "      <td>0.870318</td>\n",
       "      <td>00:14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.803009</td>\n",
       "      <td>0.856235</td>\n",
       "      <td>00:12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.783738</td>\n",
       "      <td>0.858033</td>\n",
       "      <td>00:14</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = CollabNN(*embs)\n",
    "learn = Learner(dls, model, loss_func=MSELossFlat())\n",
    "learn.fit_one_cycle(5,5e-3,wd=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.961325</td>\n",
       "      <td>0.978483</td>\n",
       "      <td>00:18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.899903</td>\n",
       "      <td>0.904707</td>\n",
       "      <td>00:17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.842807</td>\n",
       "      <td>0.867242</td>\n",
       "      <td>00:16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.748023</td>\n",
       "      <td>0.845886</td>\n",
       "      <td>00:16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.742800</td>\n",
       "      <td>0.848701</td>\n",
       "      <td>00:16</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Of course, also an easy way.\n",
    "learn = collab_learner(dls, use_nn=True, y_range=(0,5.5), layers=[100,50])\n",
    "learn.fit_one_cycle(5,5e-3,wd=.005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "SuggestedLRs(lr_min=0.017378008365631102, lr_steep=0.00363078061491251)"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEKCAYAAAAfGVI8AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8vihELAAAACXBIWXMAAAsTAAALEwEAmpwYAAAyFElEQVR4nO3dd3zV9dn/8deVcZKc7JBAQgZ7yiaCiigoioq7te6tiHXWWVu9bb3t3f5aa6u27r0HWgVXFZUlCCRsiOwVCGTvnfP5/XFOQghJSCDffE9yrufjkYdwzvec8z4x5DqfLcYYlFJK+S4/uwMopZSylxYCpZTycVoIlFLKx2khUEopH6eFQCmlfJwWAqWU8nEBdgdor9jYWNO3b1+7YyilVJeSnp6ea4yJa+6+LlcI+vbtS1pamt0xlFKqSxGRXS3dp11DSinl47QQKKWUj9NCoJRSPk4LgVJK+TgtBEop5eO0ECillI/zmUJwoLiS/27YT2lVrd1RlFLKq1hWCEQkWUR+EJGNIrJBRO5q5pqhIrJURKpE5D6rsgCs2JnPLW+ls7egwsqXUUqpLsfKBWW1wL3GmJUiEg6ki8i3xpiNja7JB+4ELrQwBwChDvdbLa/WFoFSSjVmWYvAGJNljFnp+XMJkAEkNrkm2xizAqixKkc9p8MfgPLqOqtfSimlupROGSMQkb7AWGDZUT5+poikiUhaTk7OUWUIDXK3CMp0jEAppQ5heSEQkTDgY+BuY0zx0TyHMeZFY0yqMSY1Lq7ZPZOOSFsESinVPEsLgYgE4i4C7xhjPrHytY6koUWgYwRKKXUIK2cNCfAKkGGMedKq12mrEE+LoEJbBEopdQgrZw1NAq4G1onIas9tvwNSAIwxz4tIPJAGRAAuEbkbGH60XUitcQa6C0FZlRYCpZRqzLJCYIxZDMgRrtkPJFmVobEAfz+CAvx0+qhSSjXhMyuLwT1OoGMESil1KJ8qBE6HP+XaNaSUUofwvUKgg8VKKXUIHysE2jWklFJN+VQhCA3SFoFSSjXlU4XA6QjQLSaUUqoJnyoEoTpGoJRSh/GpQhDiCNBCoJRSTfhUIXC3CLRrSCmlGvOpQuAMcrcIXC5jdxSllPIaPlUIQus3nqvR7iGllKrnU4XAqVtRK6XUYXyrEATqVtRKKdWUTxWC0CDdiloppZryqULgdLi7hnTmkFJKHeRThaChRaBdQ0op1cCnCkFDi0C3mVBKqQY+VQhCG7qGtEWglFL1fKoQ1B9gr2MESil1kE8VAh0jUEqpw/lUIQgO8EdExwiUUqoxnyoEfn6CM9BfWwRKKdWITxUCOLjxnFJKKTffKwS6FbVSSh3CBwtBgG4xoZRSjfhcIdDDaZRS6lA+VwicQQE6WKyUUo34XCEIdfhToS0CpZRq4HOFIMThr2MESinViM8VglBHgI4RKKVUIz5XCJxBuqBMKaUas6wQiEiyiPwgIhtFZIOI3NXMNSIiT4vIVhFZKyLjrMpTL9QRQHWti9o6l9UvpZRSXYKVLYJa4F5jzHDgBOA2ERne5JqzgUGer5nAcxbmAdwLygDKa7RVoJRSYGEhMMZkGWNWev5cAmQAiU0uuwB407j9BESJSIJVmaDx4TRaCJRSCjppjEBE+gJjgWVN7koE9jT6eyaHF4sOdXArah0wVkop6IRCICJhwMfA3caY4qN8jpkikiYiaTk5OceUR1sESil1KEsLgYgE4i4C7xhjPmnmkr1AcqO/J3luO4Qx5kVjTKoxJjUuLu6YMoU6tEWglFKNWTlrSIBXgAxjzJMtXDYHuMYze+gEoMgYk2VVJnBvMQFQoVNIlVIKgAALn3sScDWwTkRWe277HZACYIx5HvgSOAfYCpQD11uYB9AWgVJKNWVZITDGLAbkCNcY4DarMjSn4QB7HSNQSinAB1cWh3oGi7VFoJRSbj5XCJye6aN6XKVSSrn5XCFw+PsR4Ce68ZxSSnn4XCEQEZy6FbVSSjXwuUIA7kVl2iJQSik33ywEuhW1Uko18MlCEOoIoLxKWwRKKQU+WgicDn+dNaSUUh4+WQhCgwK0ECillIdPFoIQh78uKFNKKQ+fLAShDn/dYkIppTx8shA4HQHaIlBKKQ+fLAShQf5UVNfh3vMO6lzG5kRKKWUfnywETkcAtS5DdZ0LYwwXPfsjf/4yw+5YSillCyvPI/BazkZbUafvL2BtZhEhgf42p1JKKXv4ZIug8VbU7yzbDcDewgo7IykvU13rorbOZXcMpTqFb7YIPFtR784r57/r9xMU4EdWUSW1dS4C/H2yNqpGVu4u4NpXl1NaVUtkSCAxoQ4mD4zlttMG0jM82O54SnU4n/ytV98ieH3JTmpdhmtO7EOdy3CgpMrmZMpuewsrmPlmOtFOB3eeNojzRvWmf2wYby/bzZS/zefJbzZRUlljd0ylOpRvtgg8YwTfZhzghP4xnDI4jpcW7SAzv5zEqBCb0ym7lFXVctMbaVTV1vH+zIkM7BnecN+O3DKe+GYTT3+/lS/WZfHVXafgCPDJz1GqG/LJn+TQIHf9MwaunNin4Ze/jhP4rqLyGu56fzWbD5Tw7yvGHVIEAPrFhvLvK8bx/FXj2ZZTxptLd9oTVCkL+GSLoP4A+x6hDqYfF4/Ls54gs0ALgS/5cWsuLy3azqb9JWQVVQLw2AXHccrguBYfc9aIeE4ZHMfT323hF+OSiA51dFZcpSzjky2CyJBAAC5JTcYR4EdwoD9x4UHs1ULgM1btLuDGN1aweX8JJ/TvwUNnD+WDmSdwzYl9j/jY358zjNKqWp7+fov1QZXyqF8AawWfbBHEhgXx5g0TOL5vTMNtSdEhZBaW25hKdZZdeWXc9EYaceFBfHLrJOLCg9r1+CHx4Vx6fApvLd3F1Sf0oX9cmEVJlTpo/OPzuPqEPvzmjMEd/tw+2SIAOGVwXEMXEUBiVIi2CHxAflk11722ApcxvHH9hHYXgXr3nDGYoAA//vRFBhW6pbmyWHl1Lfll1QRbtPDVJ1sEzUmMDuGbDQdwuQx+fmJ3HNVBiitr+HDFHn7eX8KuvDK2ZJdSUV3HuzdPPKZP8nHhQdx22kD++vUmRv/xG8b3iWby4FiuObEvYUH6z0p1rBzP1Paj/eByJPoT65EU7aS6zkVOaRW9InTRUFdXXFnDa4t38sri7RRX1tIrIog+PUI5c3gvLhqbxPg+MUd+kiO49dQBjEyMZNGWXBZtyeWvX2/iu4xs3rhhghYD1aG0EHSSJM8U0syCCi0EXVxGVjGXv/QTheU1nDG8F3edPogRiZEd/joiwuRBcUwe5J5l9PX6LG57dxXXv7ac16+f0DBNWaljVV8IelpUCHx2jKCppOj6QqADxl2ZMYbHv9iIAHNvP5mXrkm1pAg056wRCTx92VhW7i7k+tdXUK5nXqgOkm1xi0ALgUditC4q82ZFFTUs2ZbLq4t3MGfNPnbllTU7nW7hllx+3JrHHacNYmRS5xSAxmaMSuCfl44hbWc+N7+ZRmWNDiSrY5dTUoW/nxDjtGbdirZdPZyOAGJCHd1uUVlReQ0B/tJluyl25pYx8600Nh8oPey+KGcgF49N4nfnDCXA3w+Xy/CXr34mOSaEK09IsSGt23mje1NT5+KeD9dw53urePbKcbqZoTomOSVVxIY5LJvI0jV/O1iku00hNcZwyQtL6BEaxLs3T0Ska82GMsbwyGfrySqs5P7pQxiRGMmw+HCyS6pYt7eIpdvyePXHHezOL+eZy8fy9YYsMrKKeeqyMQQF2Hu+xMXjkiiuqOEPczfywMdreeKXo3U2mjpq2SWVlnULgRaCQyRGhbAlu6RTXuvZ+VspqazlwbOGWvYay3fkez5Jl7J0ex4nDYi17LWs8M3GAyzaksuj5w3n+kn9Gm7vGRHMiMRILp+QwvF9o/mfORu46pVl7C+qZERiBOeN6m1j6oOum9SPoopa/jFvMy6X4d4zh5Ac47Q7luqCckqriAuzrhBY1l4VkVdFJFtE1rdwf7SI/EdE1orIchEZYVWWtkqKDmFvYYWlS7kBautcvLhwO68s2mHplsYfpO0hPCiAnuFB/HPeFsvfV0eqrKnjsbkbGdIrnKtP6NPidVef2JdnrxjHuswi9hZW8NuzhnnVJ+87Tx/IHacN5PO1WZz6tx/49TvprNpdYHcs1cVkF1dZehaGlR2XrwNntXL/74DVxphRwDXAUxZmaZPE6BAqa1zklVVb+jordhZQWF5DdZ2L73/OtuQ1iitr+HJdFueN6c2vpwxg+Y58lm7Ps+S1rPDs/G3sLazgjxccd8T+9bNHJvDezBN47ILjOHmQd7V6RIR7zxzC4gdP45ZTB7B4Sy4XPbuEez5cTV6pnn+hjqzOZcgrq7a0a8iyQmCMWQjkt3LJcOB7z7U/A31FpJdVedoiKdrdbLd6nGBexgEcAX7Ehjn4ev1+S15j7pp9VNa4uDQ1mcsmpHh9qyC7uJJvNx5g7pp9vLd8N88v2Mb5o3tzQv8ebXr8+D7Rbdowzi7xkcE8eNZQlj50OrdPHcic1fuY9uQCPkrb47X/T5R3KCivps5luu0YwRrgYmCRiEwA+gBJwIGmF4rITGAmQEqKdbNBEhstKhudHGXJaxhj+HbjASYN6EFStJOP0vdQXl2L09G+/xUFZdXsyCtjXEp0s/d/uGIPQ+PDGZUUiYjw6ykD+MPcjV47VnD7e6tYvuPg54YeoQ5+d84wGxNZIzQogPumD+H8Mb353SfruH/2WhwBflwwJtHuaMpLZRdbu5gM7F1H8BcgSkRWA3cAq4BmJ10bY140xqQaY1Lj4lreK/5YHVxLYN2iss0HStmdX84Zw+M5e0Q8lTUuFmzKaffz/GPeZn7x3BK+Xp912H0ZWcWsySziV6nJDTOF6lsFj362gdnpmQ3dEjklVfxnVSb/+/lGNu4rPrY3d5TySqtYsTOfa0/sw7e/OYVFD0xl0YNTiY/sviu8B/cK58NbTmRgzzBeWLBdWwWqRTml1i4mAxtbBMaYYuB6AHH/ttoBbLcrD7jPKQgPDrB0LcG3G91dQdOG9SQm1EG0M5Cv1u/n7JEJzV6/J7+c+MhgApv0ky/cnIMxcPcHq/kgMuSQFswHK/bg8PfjorEHP2UGB/rzvxeO4JFP13PfR2sQgeRoJ7vz3UVPxP24l65J5cQBbeuO6Sg/bHK/l1+OT2ZQr/AjP6Cb8PMTbp7cjwc/XsePW/O8bnxDeQer9xkCG1sEIhIlIvXL5G4CFnqKg62Sop2WjhF8u/EAo5Oj6BkRTIC/H2cOj+f7n7Opqj28MfT1evdMk398u/mQ2/fkl7Mzr5zbpg4gNiyIm95MY29hBQVl1by8aDsfp2dy5nG9Djs9a/px8fz00OnMvf1k7jxtEEPiw7l/+hDm3n4yix88jfjIYK59bTn/3WDNuEVL5m08QK+IIEYkRnTq63qDC8YkEhsWxIuLbP0MpLxYdon79LwuWQhE5D1gKTBERDJF5EYRmSUiszyXDAPWi8gm4GzgLquytId7LUGpJVsDHCiuZE1mEWcOPzgmftbIeEqralm8JfeQaxduzuGO91bhMjBnzb5Dug4Wb3Vfe+GYRF697ngqq+u46N8/MvH/vuPxLzIY0DOsxcMr/PyEkUmR/OaMwbx0TSq3TR3IyKRIEqNC+OiWExmeEMGtb6czd82+Dn//zamsqWPhlhxOH9aryy146wjBgf5cP6kvCzfnkJFl++cg5YVySqoICwpo9zhie1g5a+hyY0yCMSbQGJNkjHnFGPO8MeZ5z/1LjTGDjTFDjDEXG2O8YnL1uaMS2J1fzqUv/kR2cWWHPve8DPc4+BmNCsGkAbGEBwfwVaPZQ2k785n5VhoDe4bz8IxhZBZUsKFR//3iLbn0ighiYM8wBvcK57mrxuN0+HP5hGS+umsyn942iQFHsdd+dKiDd2+eyOjkKP44d0OnbJr20/Y8yqvrOGOYrRPGbHXlxBRCAv15SVsFqhk5JVWWDhSDbjp3mAvHJvLC1ePZcqCEC/79I+v3FnXYc3+z4QB9ejgZ1PPgL2lHgB/ThvXi01V7Ofn/fc/UJ+Zz9SvLSYgM4c0bJvCLcUn4+wlfeQaFXS7Dj9tyOXlgXMMn6JMHxTL//qn88YIRDEs4tu4VpyOAh2cMI7e0mjeW7Dqq56ipc7W5RfVdRjYhgf6dPi7hTaKcDi49Ppk5q/eRVdR9tjhRHSO7pIpYbygEIhIqIn6ePw8WkfNFJNDSZDaaflw8s2edhACXvrCUovJjW/27v6iSWW+ls2BzDueOSjisC+S2qQO4JDWJCX1jGJEYyYxRCbx900TiwoOIDnVwQv8Yvlq3H2MMG/YVU1hew2QLBxbH94lhypA4Xli4rd0rn40x3PD6Cib8aR5vLNlJbZ2r1Wu/yzjA5EGxlh3B11XceHI/XMbw2NyN1Ll0BpE6KLekytLxAWh7i2AhECwiicA3wNW4Vw53W8N7R/D4RSMoq65ja87R7z/01tKdTHtyAT9syub+6UO46/TD++4H9gznzxeP4slLx/DM5WN54pLRDWsawL3P/fbcMjYfKGXRVvdU05MGWvsJ+p4zBlNYXsNrP+5suG3xllx+9591lFa13GX0wYo9LNqSS8+IYB6ds4Fzn1nMshZWNG/MKmZfUSXTfLhbqF5yjJPfnj2Ur9bv57cfr8WlxUB5eFPXkBhjynEvAHvWGHMJcJx1sbxDSkwoQMMUy/ZK31XAI59tYExyFN/85hRumzoQR0D7e+OmH9cLEfhqfRaLt+QyND7c0n1HAEYlRXHG8F68tGg7uaVV/PmrDK56ZRnvLtvN7/+zrtl57/uLKvnTFxmc2L8H39x9Cs9dOY6SylouffEn/v7NpsM+6c7bmI0ITB3a09L30lXMPGUAd54+iI/SM3ns8426tkBRUV1HSVWt5S2Ctg5Di4icCFwJ3Oi5rdu35etPLduTf3T9tku25iIC/7piLFHHcKBEz/Bgju8Tw5w1+8jMr+CaE1vehK0j3XPGYM5+ahGnPTGf4sparpiYQozTwb9+2MqkAbH86vjkhmuNMTz86XpqXC7+8ouR+PkJZ49MYMqQnjw6Zz3PfL+V1XsKeeqysYQE+rNiZz6frd7LmOQoy3/Iu5LfTBtEeVUtLy/eQWRIYIuzv5RvaFhDYOHOo9D2QnA38BDwH2PMBhHpD/xgWSovERzoT8/wIPYcZYvgpx15DI2POKYiUO+sEfE89vlGgE5beDQsIYKLxyYyL+MAz105jrNHJlDnMqzcXcD/zFnPmJQoBnsWgM1dm8W8jAM8PGMYfXqENjxHiMOfv/5yNOP7RPPIZxs47e/zKa+uo7rWRaC/cPcZYzrlvXQVIsLvZwyjsKKGp77bwoR+MUwaqAvNfFVOqXvmYk+Lz1FvUyEwxiwAFgB4Bo1zjTF3WhnMWyTHONlzFOcYV9e6SN9VwOUTOmZvpPpC4PD3Y0K/mA55zrb46y9HUesyDYO5/n7CPy8dwzlPL+LWt9OZ2L8HP23PY3tOGaOTow45N6CxS49P4bjekTz93Rb69HAyaWAsE/rFWDo3uqsSEf73ghGs2l3APR+u5uu7TjlscaDyDfX7DFndImjrrKF3RSRCREKB9cBGEbnf0mReIiXGeVRdQ2szC6mscTGxX8cM6vaOCmFivxgmDezRqb88A/z9DpvR0zMimH9cOoYduWXMXb2PPjFOHjp7KK9cm4p/K2cBjEiM5MVrUvn9jOFMGdJTi0ArQhz+PHXZWPLLqvntJ2t1vMBHdcY+Q9D2rqHhxphiEbkS+Ar4LZAO/M2yZF4iOTqEz1ZXUFPnOmy/n9b85JkpM7EDP72/et3xeMvi28mD4lj++2lEhQTqebwWGZEYyQPTh/KnLzP4YMUeLuug1qXqOhoOrbe4RdjWf8GBnnUDFwJzjDE1gE98REmKceIysK+wfa2Cn7bnMzQ+vEOb9KEWLzNvr9iwIC0CFrvx5H5MGtiDxz7f2LDnjPId2cVV9Ah1tNrS7ght/Vf8ArATCAUWikgfwCc2RknxnDHbdArpzDfTeLmFLQGqa12k7cpv86EqSrXEz0/404Ujqap18ewP2+yOozpZTqn1i8mgjYXAGPO0MSbRGHOOcdsFTLU4m1eoP2y88ThBYXk132w8wNPfbaGsmcVV9eMDWghUR+gbG8ol45N4d9lu9razZaq6tuySSssXk0HbB4sjReRJEUnzfP0dd+ug24uPCCbQXw6ZObTOs/9QcWUtH6zYc9hjrBgfUL7tjtMHAfDMd1tsTqI6U04nbC8Bbe8aehUoAX7l+SoGXrMqlDfx9xN6R4UcspZgbaa7EAxPiOCVxTsO20/HivEB5dsSo0K4YmIKH6VnsiO3zO44qhO4XIbcUmsPra/X1kIwwBjzqDFmu+frj0B/K4N5E/cU0saFoJC+PZzcPW0QewsrDtlCWscHlFV+PXUAgf7CU/M2H/li1eXlew6tt3o7GWh7IagQkZPr/yIikwCf6axMinayp9GpZesyixiVFMW0Yb3oHxvKiwsPnjm7cneBjg8oS/QMD+a6k/rx2Zp9pO3MtzuOslj99hKxFi8mg7YXglnAv0Vkp4jsBP4F3GJZKi+THBNCflk1pVW15JRUsa+oklFJkfj5CTdN7s+6vUV8tnoff5izgeteW05YUICODyhL3HrqAPr2COWmN9PYllNqdxxloWxPIegZ4SWFwBizxhgzGhgFjDLGjAVOszSZF0lpmDlUzrq9hQCMTIwE4OJxifQIdXD3B6t5+6ddnDuqN3Nun6TjA8oSkc5AXr/+ePxFuPbV5bq2oBurbxF4zayhesaY4kYHzN9jQR6vlBx9sBCs2VOEiHvVJ7g3pnv8whHcdHI/frhvCk9cMpr+R3FMpFJt1adHKK9edzx5pdXc+Hpas1OYVdfXGYfW1zuWZaFestmB9RrWEhRUsG5vEQPjwggNOrjC9+yRCTx87vCG65Sy2ujkKP595Vg27Cvi6e91Sml3lF1s/aH19Y6lEPjEFhMA0c5AQh3+7MkvZ61noFgpu502tBfThvVidlom1bUtHwmquqacUutPJqvXaiEQkRIRKW7mqwTo3SkJvYCIkBzjZPmOfHJLqxiVFGl3JKUAuGxCMnll1XyXccDuKKqD5RR3zmIyOEIhMMaEG2MimvkKN8Z4z+5nnSA5xsnGLPfwiBYC5S1OHdyT+Ihg3m9mhbvq2rJLKr2jEKiD6geMA/yEYQkRNqdRys3fT/hVahILt+ToPkTdTHZJVacsJgMtBG2WEuM+v3hIfPhhB7UoZadLUt1nR3+Upq2C7qKsqpby6rpOWUMAWgjarH5GkHYLKW+THOPk5IGxfJSWSZ3LZ+ZwdGvZnbiGALQQtFn92oCxydE2J1HqcJcdn8LewgoWbcmxO4rqANnFnbeGALQQtFm/2FA+mnUiF49LtDuKUoeZNrwnMaEOPkrLtDuK6gD1ZxXrGIEXOr5vjB7NqLxSUIA/04/rxcLNOdTU6ZqCri67WLuGlFJH4dTBcZRU1bJyV4HdUdQxyi6pItBfiHIGdsrraSFQqps4aWAsAX7Cgs06TtDV5ZRUERcWhEjn7OSjhUCpbiIiOJBxfaKZv0kLQVeXXVJJXETnjA+AhYVARF4VkWwRWd/C/ZEiMldE1ojIBhG53qosSvmKKUPi2JhV3DDrRHVN9S2CzmJli+B14KxW7r8N2Og552AK8HcR0U38lToGpw6OA9DuoS4up6Sq0xaTgYWFwBizEGjtPD0DhIu7EyzMc61urK7UMRieEEFceJAWgi6sps5FXll1p80YAnvHCP4FDAP2AeuAu4wxzc57E5GZIpImImk5OfoDrlRLRIRTB8exaEsutTqNtEvK9awh6KzFZGBvIZgOrMa9nfUY4F8i0uxubsaYF40xqcaY1Li4uM5LqFQXdOrgOIoqaliTWWR3FHUUDq4h6AaDxW1wPfCJcdsK7ACG2phHqW5h8qBY/ETHCbqqzjyruJ6dhWA3cDqAiPQChgDbbcyjVLcQ5XQwJjmKBZuy7Y6ijkL9hnPdomtIRN4DlgJDRCRTRG4UkVkiMstzyf8CJ4nIOuA74EFjTK5VeZTyJdOG92JNZhFbs0vsjqLaqf7Q+thOnD5q2SljxpjLj3D/PuBMq15fKV92aWoy/5y3hVd/3Mn/XTTS7jiqHXJKqogJdeAI6LwOG11ZrFQ31CMsiIvHJvLJykwKyqrtjqPaIbuTF5OBFgKluq3rJ/WjssbFu8t32x1FtUN2Jy8mAy0ESnVbQ+LDmTwoljeW7KS6VtcUdBW5JVWdOlAMWgiU6tZuOLkf2SVVfLkuy+4oqg2MMe59hrQQKKU6yqmD4hgQF8ori3dgjJ5n7O0Ky2uornN16mIy0EKgVLfm5yfccHI/1u0tYtmO1rb+Ut7g4BGV2iJQSnWgX4xLIjbMwfMLttkdRR1B/fYS2jWklOpQwYH+XHdSX+ZvyiEjq9juOKoV9YvJtEWglOpwV5/Ql1CHPy9oq8CrHajfcK4TTycDLQRK+YRIZyBXTExh7tos9uSX2x1HtWBvYTlRzkDCgizb9KFZWgiU8hE3ntwfP4GXF+nejt4qs6CCxKiQTn9dLQRK+Yj4yGAuHJPIB2l7yPPMTlHeJbOggqRoLQRKKQvdcmp/KmtcfJC2x+4oqgljDHsLKkiKdnb6a2shUMqHDOwZzriUKD5foyuNvU1+WTUVNXXaIlBKWe+ckQlszCpmR26Z3VFUI5kFFQA6RqCUst45IxMAdP8hL7O30F0ItGtIKWW53lEhjO8TzedrtRB4k8wC97TeRO0aUkp1hhkjE8jIKmZbTqndUZRHZkEFEcEBRIYEdvprayFQygedPTIegC+1VeA1MgsqSLShWwi0ECjlkxIiQ0jtE80XHThOkFtaxVtLd1JTp4fgHI29Nq0hAC0ESvmsGaMS+Hl/CVuzO6Z76A9zNvDIZxt4+D/r9eyDdjLGkFlQroVAKdW5zh6RgAg8/sVGnp2/lbd/2sXazMKjeq61mYV8vjaLgT3D+CBtD8/O183t2qOwvIay6jpbZgyBFgKlfFZ8ZDDnjurN4i25/PXrTTz86XouenYJS7bmHnLdztwyLnl+CZ+v3dfs8xhj+MtXPxMT6uCTX5/EBWN687f/buKz1Xs74210C3auIQDo3C3ulFJe5ZnLx2IuG0NljYu8siquf20Ft76zkk9vm0S/2FD2FVZw5cvL2FtYwcrdhQT4CWeNSDjkORZuyWXJtjwePW84EcGB/PWXo8gqrOT+j9YyIC6MEYmRNr27rmNvoXvqqHYNKaVsISKEOPxJinbyyrXH4ydw4xsr2JZTylUvL6O4ooYPZp7A6KRI7nhvFd9lHGh4rMvlbg0kx4RwxcQUAIIC/Hnh6vE4g/z557wtdr2tLqW+RZBsU9eQtgiUUg1Sejh57qrxXPXyMqb/YyGB/n68fdMExveJ4fUbJnDVy8u49e2VXDExhQA/Iae0ioysYp66bAxBAf4NzxMd6uDaE/vy1Hdb2LS/hCHx4Ta+K++XWVBBWFAAESH2/ErWFoFS6hAn9O/B/100ksiQQF6+NpXxfWIAiAgO5M0bJjAmOYoP0/bw7vLdzNt4gGnDenHeqN6HPc91J/XF6fDnuflbO/stdDn1M4ZExJbX1xaBUuowvzo+mUtSkw77xRTldPDhrBPb9BzRoQ6umJDCa0t2cs8ZQ0jpYU+3R1dg1zkE9bRFoJRqVkd8Or1pcn/8RXhhoU4nbYmd5xDU00KglLJMfGQwvxifxEfpmWQXV9odxysVV9RSUlVr29RR0EKglLLYrFP7U1vn4iU9K7lZmTZPHQULC4GIvCoi2SKyvoX77xeR1Z6v9SJSJyIxVuVRStmjT49QLhiTyFs/7SK7RFsFTdVPHe2uXUOvA2e1dKcx5m/GmDHGmDHAQ8ACY0y+hXmUUja58/RB1NQZnp+vrYKmDhaCbtgiMMYsBNr6i/1y4D2rsiil7NUvNpSLxyby9rJdHNCxgkNkFpTjdPgT5ez8cwjq2T5GICJO3C2Hj+3OopSyzh2nDcLlMjz7g64raGxXXjnJ0U7b1hCAFxQC4Dzgx9a6hURkpoikiUhaTk5OJ0ZTSnWUlB5OLklN4r3le9hXWEFuaRWz0zN5YcE2XC7f3LbaGMPqPYWMTLJ3PyZvWFB2GUfoFjLGvAi8CJCamuqbPzFKdQO3nzaI2emZnP+vH8krq6L+2IKgAD+um9TP3nA22JlXTn5ZNeP7RNuaw9YWgYhEAqcCn9mZQynVORKjQrjr9EH07eHkN9MG8/kdJzN1SBz/7+tN7Morsztep1u5qwCAcSndtBCIyHvAUmCIiGSKyI0iMktEZjW67CLgG2OM7/0EKOWjbj9tELNvPYk7Tx/EiMRI/nzxKAL8hftnr/W5LqL03QWEBwUwqGeYrTks6xoyxlzehmtexz3NVCnlo+Ijg3nk3OE8MHstby7d6VNdRCt3FTAmJQo/P/sGisE7BouVUj7ukvFJDV1Ee/LL7Y7TKUoqa9h0oMT28QHQQqCU8gIiwv9dPJI6Y3jWR7atXr2nEGPQQqCUUvUSIkO4ZHwSH6fv9YlFZyt3FSICY5Kj7I6ihUAp5T1uOWUAdcbwsg9sUJe+u4AhvcIJD7ZvRXE9LQRKKa+R0sPJeaMSeGfZbgrLq+2OYxmXy7BqdwFjbZ42Wk8LgVLKq9w6ZSDl1XW8vmSn3VEsszWnlJLKWq8YHwAtBEopLzMkPpxpw3ry+pKdlFXV2h3HEumehWRaCJRSqgW/njqQwvIaHvpkHXmlVXbH6XArdxUQE+qgr5ec46yFQCnldcalRHPb1AF8sS6LKU/M5+VF26muddkdq8Ok7ypgXEqUrTuONqaFQCnlle6fPpSv75rM2JRoHv8ig5vfTMOYrr8FxeYDJWzPLePkgbF2R2mghUAp5bUG9QrnjeuP54GzhrBgcw5Lt+XZHemYzVm9Dz+BGaN62x2lgRYCpZRXExFumNSPhMhg/v7t5i7dKjDG8NmavUwaGEtceJDdcRpoIVBKeb3gQH9umzqQ9F0FLNjcdQ+nWrm7kD35FVwwJtHuKIfQQqCU6hJ+lZpMUnQIT3bhVsGc1XsJCvBj+nG97I5yCC0ESqkuwRHgx52nDWJtZhHzMrLtjtNutXUuPl+bxbRhvbxiW4nGtBAopbqMi8cl0qeHkye/3UxtnT3TSStr6rj3wzV8tnpvu1omP27LI6+smvPHeM8gcT0tBEqpLiPA348Hpg8lI6uYZ763Z7vq7zKy+XhlJne9v5qZb6WT3cadUj9bvZfw4ACmDImzOGH7aSFQSnUpM0YlcPHYRJ75fgvLd+R3+ut/vnYfsWFBPHT2UBZuzuGMfyzkwxV7Wj1ms6yqlv+u3885IxIICvDvxLRto4VAKdXlPHbhCJJjnNz9/iqKymuO+nkqa+q4471VPDB7DW//tIv1e4ta7XIqrarl+5+zmTEynltOHcCXd01mUM8wHvh4Lef/e3GzhckYwwMfr6Wipo7LJiQfdVYraSFQSnU5YUEBPH3ZWLJLqnjoP2uPehbRc/O3MXfNPr7ZeICHP13Puc8s5txnFrM1u6TZ6+dtPEBVrYtzR7v7+QfEhfHRrBN56rIx5JVW86sXlnLHe6vILzu4hfYLC7fzxdos7p8+1Gu2nW5KC4FSqksanRzFvWcO4ct1+7no2SUs3pLbroKwK6+M5xZs4/zRvVn1yBksemAqf/3lKHJKqjj3mcW8v3z3Yc/3+dp9xEcEM77RL3QR4YIxiXx/7xTuOn0QX6/P4sx/LODbjQdYuDmHv379MzNGJTDr1P4d9t47mnS1+bipqakmLS3N7hhKKS/gchk+SNvD099tIauokon9YvjLL0bRLza01ccZY7jh9RUs35HP9/dNoVdEcMN92cWV3PPhGhZvzeWCMb154pLRBPr7UVRRQ+rj33LtiX15+NzhLT53RlYx93y4hoysYhwBfvSPDeWTX5+E0xHQYe/7aIhIujEmtbn7tEWglOqy/PyEyyek8MN9U/jDecP5eX8Jt72zkpom/fxz1+zjpjfS+HbjAepchnkZ2fywKYffnDH4kCIA0DMimDdvmMC9Zwzms9X7eGD2Wlwuwzcb9lNTZxq6hVoyLCGCz26bxO1TB9KvRygvXD3e9iJwJN6dTiml2iA40J/rJvUjPjKYWW+v5Pn527jj9EEArMss4t6P1mCMYV7GAVJinFTV1jG4VxjXntS32efz85OGx//9281EBAewM6+c5JgQRidFHjGPI8CP+6YP4b7pQzrsPVpJC4FSqts4a0QCM0Yl8PT3WzjzuHh6hgdx6zvpxIY6+PS2SazYWcBrP+5g9Z5Cnr5sLIH+rXeK3H7aQIoqanh58Q4AZp06wGvOEOhIWgiUUt3KY+cfx9Jtedw/ew3RTgfZxVV8OOtEekYEM2OUu1BUVNcR4jjyfH4R4fczhlFSWcvslZlc4IWrgjuCDhYrpbqdz9fu4/Z3VwHw+IUjuOqEPsf0fMYYskuqDhtP6EpaGyzWFoFSqtuZMTKBlZMKcQT4ceXElGN+PhHp0kXgSLQQKKW6HRHhf85reYqnOpROH1VKKR+nhUAppXycFgKllPJxlhUCEXlVRLJFZH0r10wRkdUiskFEFliVRSmlVMusbBG8DpzV0p0iEgU8C5xvjDkOuMTCLEoppVpgWSEwxiwEWjs14grgE2PMbs/1Xe8QUqWU6gbsHCMYDESLyHwRSReRa1q6UERmikiaiKTl5OR0YkSllOr+7CwEAcB4YAYwHXhERAY3d6Ex5kVjTKoxJjUuzvvO+1RKqa7MzgVlmUCeMaYMKBORhcBoYHNrD0pPT88VkV2ev0YCRa38ueltgUBuO3M2fo623Nf0trZmrP9vbDszdla++tv0e+hd+bpCRm/PdywZW7vN276HLe+zYYyx7AvoC6xv4b5hwHe4i5ETWA+MaOfzv9jan5veBqQdxXt4sT33Nb2trRkb/bddGTsrn34PvTNfV8jo7fmOJeMRsnrV97C1L8taBCLyHjAFiBWRTOBR3J8mMcY8b4zJEJGvgbWAC3jZGNPiVNMWzD3Cn5veNq6dz9/0OdpyX9Pb2pqxtddpTWflq/+zfg+9K19L93tTRm/P19L9bcl4pNvaw+rvYYu63O6jx0JE0kwLu+95C2/P6O35wPszens+8P6M3p4PukbGer62svhFuwO0gbdn9PZ84P0ZvT0feH9Gb88HXSMj4GMtAqWUUofztRaBUkqpJrQQKKWUj9NCoJRSPk4LgYeITBaR50XkZRFZYnee5oiIn4j8SUSeEZFr7c7TlGc32UWe7+MUu/M0R0RCPduVnGt3luaIyDDP92+2iNxqd57miMiFIvKSiHwgImfanacpEekvIq+IyGy7s9Tz/Ny94fm+XWl3nqa6RSFoactrETlLRDaJyFYR+W1rz2GMWWSMmQV8DrzhjRmBC4AkoAb3ymxvy2eAUiDYS/MBPAh82JHZOjKjMSbD83P4K2CSl2b81BhzMzALuNQL8203xtzYkbma086sFwOzPd+3863O1m7tWfnmrV/AKbgXOq1vdJs/sA3oDziANcBwYCTuX/aNv3o2etyHQLg3ZgR+C9zieexsL8zn53lcL+AdL8x3BnAZcB1wrjf+P/Y85nzgK+AKb83oedzfgXFenK9D/40cY9aHgDGea961MtfRfHWLw+uNMQtFpG+TmycAW40x2wFE5H3gAmPMn4FmuwVEJAUoMsaUeGNGzwrtas9f67wtXyMFQJC35fN0V4Xi/odZISJfGmNc3pTR8zxzgDki8gXwbkfl66iMIiLAX4CvjDErvS1fZ2lPVtwt5CRgNV7YE9MtCkELEoE9jf6eCUw8wmNuBF6zLNHh2pvxE+AZEZkMLLQymEe78onIxbh3ko0C/mVpMrd25TPG/B5ARK4DcjuyCLSivd/DKbi7EYKAL60M1kh7fw7vAKYBkSIy0BjzvJXhaP/3sAfwJ2CsiDzkKRidpaWsTwP/EpEZHP0WFJbpzoWg3Ywxj9qdoTXGmHLcxcorGWM+wV2svJox5nW7M7TEGDMfmG9zjFYZY57G/YvNKxlj8nCPX3gN495l+Xq7c7TE65ooHWgvkNzo70me27yJt2fUfMdOMx47b8/XWFfK2qA7F4IVwCAR6SciDtyDhHNsztSUt2fUfMdOMx47b8/XWFfKepDdo9Ud8QW8B2RxcFrljZ7bz8F90M024PeaUfNpRu/O6O35umrWI33ppnNKKeXjunPXkFJKqTbQQqCUUj5OC4FSSvk4LQRKKeXjtBAopZSP00KglFI+TguB6hZEpLSTX69DzqwQ9xkORSKyWkR+FpEn2vCYC0VkeEe8vlKghUCpZolIq/twGWNO6sCXW2SMGQOMBc4VkSOdQ3Ah7h1UleoQWghUtyUiA0TkaxFJF/fJaUM9t58nIstEZJWIzBORXp7b/yAib4nIj8Bbnr+/KiLzRWS7iNzZ6LlLPf+d4rl/tucT/TuebZoRkXM8t6WLyNMi8nlreY0xFbi3KU70PP5mEVkhImtE5GMRcYrISbjPK/ibpxUxoKX3qVRbaSFQ3dmLwB3GmPHAfcCzntsXAycYY8YC7wMPNHrMcGCaMeZyz9+H4t5aewLwqIgENvM6Y4G7PY/tD0wSkWDgBeBsz+vHHSmsiEQDgzi4xfgnxpjjjTGjgQzcWxgswb13zf3GmDHGmG2tvE+l2kS3oVbdkoiEAScBH3k+oMPBw3KSgA9EJAH3KVI7Gj10jueTeb0vjDFVQJWIZOM+fa3pMZzLjTGZntddDfTFfWTndmNM/XO/B8xsIe5kEVmDuwj80xiz33P7CBF5HPf5DmHAf9v5PpVqEy0EqrvyAwo9fe9NPQM8aYyZ4zkI5g+N7itrcm1Voz/X0fy/mbZc05pFxphzRaQf8JOIfGiMWQ28DlxojFnjOUxnSjOPbe19KtUm2jWkuiVjTDGwQ0QuAffxiiIy2nN3JAf3iL/WogibgP6NjjI84iHvntbDX4AHPTeFA1me7qgrG11a4rnvSO9TqTbRQqC6C6eIZDb6ugf3L88bPd0uG3CfHQvuFsBHIpIO5FoRxtO99Gvga8/rlABFbXjo88ApngLyCLAM+BH4udE17wP3ewa7B9Dy+1SqTXQbaqUsIiJhxphSzyyifwNbjDH/sDuXUk1pi0Ap69zsGTzegLs76gV74yjVPG0RKKWUj9MWgVJK+TgtBEop5eO0ECillI/TQqCUUj5OC4FSSvk4LQRKKeXj/j+tYb2JnKsHbgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn = collab_learner(dls, use_nn=True, y_range=(0,5.5), layers=[100,50])\n",
    "learn.lr_find()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I would not change the learning rate based on this, so, while I'm sure there are more ways to improve this, I think we're good for now."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fastai",
   "language": "python",
   "name": "fastai"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "name": "ch8_collab_filtering.ipynb"
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
